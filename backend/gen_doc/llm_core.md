{
  "__init__.py": "```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model\\_selection import train\\_test\\_split\nfrom sklearn.linear\\_model import LogisticRegression\nfrom sklearn.metrics import accuracy\\_score, classification\\_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nclass TitanicClassifier:\n\"\"\"\nA class to train and evaluate a Logistic Regression model on the Titanic dataset.\n\"\"\"\ndef \\_\\_init\\_\\_(self, data\\_path):\n\"\"\"\nInitializes the TitanicClassifier with the path to the dataset.\nArgs:\ndata\\_path (str): Path to the Titanic dataset CSV file.\n\"\"\"\nself.data\\_path = data\\_path\nself.df = None\nself.model = None\nself.X\\_train, self.X\\_test, self.y\\_train, self.y\\_test = None, None, None, None\ndef load\\_data(self):\n\"\"\"\nLoads the Titanic dataset from the specified CSV file into a Pandas DataFrame.\n\"\"\"\ntry:\nself.df = pd.read\\_csv(self.data\\_path)\nexcept FileNotFoundError:\nprint(f\"Error: File not found at {self.data\\_path}\")\nself.df = None\nexcept Exception as e:\nprint(f\"Error loading data: {e}\")\nself.df = None\ndef preprocess\\_data(self):\n\"\"\"\nPreprocesses the Titanic dataset:\n- Handles missing values in 'Age' by imputing with the median.\n- Converts categorical features ('Sex', 'Embarked') into numerical using one-hot encoding.\n- Selects features for training the model.\n\"\"\"\nif self.df is None:\nprint(\"Error: Dataframe is empty. Please load data first.\")\nreturn\n# Handle missing values in 'Age'\nself.df['Age'].fillna(self.df['Age'].median(), inplace=True)\n# Convert categorical features to numerical using one-hot encoding\nself.df = pd.get\\_dummies(self.df, columns=['Sex', 'Embarked'], drop\\_first=True)\n# Select features\nself.features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex\\_male', 'Embarked\\_Q', 'Embarked\\_S']\nself.X = self.df[self.features]\nself.y = self.df['Survived']\ndef split\\_data(self, test\\_size=0.2, random\\_state=42):\n\"\"\"\nSplits the data into training and testing sets.\nArgs:\ntest\\_size (float, optional): The proportion of the dataset to include in the test split. Defaults to 0.2.\nrandom\\_state (int, optional): Random seed for reproducibility. Defaults to 42.\n\"\"\"\nif self.X is None or self.y is None:\nprint(\"Error: Features (X) and target (y) are not defined. Please preprocess data first.\")\nreturn\nself.X\\_train, self.X\\_test, self.y\\_train, self.y\\_test = train\\_test\\_split(\nself.X, self.y, test\\_size=test\\_size, random\\_state=random\\_state\n)\ndef train\\_model(self):\n\"\"\"\nTrains a Logistic Regression model on the training data.\n\"\"\"\nif self.X\\_train is None or self.y\\_train is None:\nprint(\"Error: Training data is not defined. Please split data first.\")\nreturn\nself.model = LogisticRegression(max\\_iter=1000)\nself.model.fit(self.X\\_train, self.y\\_train)\ndef evaluate\\_model(self):\n\"\"\"\nEvaluates the trained model on the test data and prints the accuracy and classification report.\n\"\"\"\nif self.model is None or self.X\\_test is None or self.y\\_test is None:\nprint(\"Error: Model or test data is not defined. Please train model and split data first.\")\nreturn\ny\\_pred = self.model.predict(self.X\\_test)\naccuracy = accuracy\\_score(self.y\\_test, y\\_pred)\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(\"Classification Report:\")\nprint(classification\\_report(self.y\\_test, y\\_pred))\ndef visualize\\_data(self):\n\"\"\"\nGenerates visualizations of the data including:\n- Survival count\n- Age distribution\n- Correlation heatmap\n\"\"\"\nif self.df is None:\nprint(\"Error: Dataframe is empty. Please load data first.\")\nreturn\n# Survival Count\nplt.figure(figsize=(6, 4))\nsns.countplot(x='Survived', data=self.df)\nplt.title('Survival Count')\nplt.xlabel('Survived (0 = No, 1 = Yes)')\nplt.ylabel('Count')\nplt.show()\n# Age Distribution\nplt.figure(figsize=(8, 6))\nsns.histplot(self.df['Age'].dropna(), kde=True)\nplt.title('Age Distribution')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()\n# Correlation Heatmap\nplt.figure(figsize=(10, 8))\ncorrelation\\_matrix = self.df.corr()\nsns.heatmap(correlation\\_matrix, annot=True, cmap='coolwarm', linewidths=.5)\nplt.title('Correlation Heatmap')\nplt.show()\ndef run(self):\n\"\"\"\nRuns the entire Titanic classification pipeline:\n- Loads the data.\n- Preprocesses the data.\n- Splits the data into training and testing sets.\n- Trains the Logistic Regression model.\n- Evaluates the model.\n- Visualizes the data.\n\"\"\"\nself.load\\_data()\nif self.df is not None:\nself.preprocess\\_data()\nif hasattr(self, 'X') and hasattr(self, 'y'):\nself.split\\_data()\nif self.X\\_train is not None and self.y\\_train is not None:\nself.train\\_model()\nif self.model is not None:\nself.evaluate\\_model()\nself.visualize\\_data()\n```",
  "api.py": "```markdown\n# CoreLLM Documentation\nThis document provides detailed information about the `CoreLLM` class, which enables interaction with various Language Model Models (LLMs) in an API style. It supports models such as `meta-llama/Llama-3.2-3B-Instruct` and `Gemini 2.0 Flash`.\n## Class Definition: `CoreLLM`\n### Overview\nThe `CoreLLM` class serves as a central interface for loading, configuring, and interacting with different LLMs. It handles model loading, prompt formatting, API calls, and response formatting.\n### Initialization: `\\_\\_init\\_\\_(self, verbose: bool = False)`\nThe constructor initializes the `CoreLLM` object with default settings, loads the prompt template, and prepares for model loading.\n#### Parameters:\n- `verbose` (`bool`, optional): If set to `True`, enables verbose output for debugging purposes. Defaults to `False`.\n#### Attributes:\n- `model\\_name` (`str`): Name of the loaded model. Initially `None`.\n- `client`: The LLM client object (e.g., `transformers.AutoModelForCausalLM` or `google.genai.Client`). Initially `None`.\n- `tokenizer`: The tokenizer object (e.g., `transformers.AutoTokenizer`). Initially `None`.\n- `prompt\\_template` (`str`): The prompt template loaded from `llm\\_core/prompt.txt`.\n- `verbose` (`bool`): Indicates whether verbose output is enabled.\n#### Usage example:\n```python\ncore\\_llm = CoreLLM(verbose=True)\n```\n### Model Loading: `load\\_model(self, model\\_name: str)`\nThis method loads the specified LLM and its associated tokenizer. It uses environment variables for API keys and local model paths.\n#### Parameters:\n- `model\\_name` (`str`): The name of the model to load. Supported values:\n- `\"meta-llama/Llama-3.2-3B-Instruct\"`\n- `\"Gemini 2.0 Flash\"`\n#### Functionality:\n- Sets the `model\\_name` attribute.\n- Uses a `match` statement to load the appropriate model and tokenizer based on `model\\_name`.\n- For `\"meta-llama/Llama-3.2-3B-Instruct\"`:\n- Loads the model using `transformers.AutoModelForCausalLM.from\\_pretrained` with 4-bit quantization and moves it to the available device.\n- Loads the tokenizer using `transformers.AutoTokenizer.from\\_pretrained`.\n- Prints the model to the console if `verbose` is `True`.\n- For `\"Gemini 2.0 Flash\"`:\n- Initializes the `genai.Client` with the API key from the environment variables.\n#### Environment Variables:\n- `LOCAL\\_MODELS`: A dictionary-like environment variable containing paths to locally stored models.\n- `HUGGINGFACE\\_TOKEN`: Hugging Face API token for accessing models.\n- `GEMINI\\_API\\_KEY`: Google Gemini API key.\n#### Usage example:\n```python\ncore\\_llm.load\\_model(\"meta-llama/Llama-3.2-3B-Instruct\")\n```\n```python\ncore\\_llm.load\\_model(\"Gemini 2.0 Flash\")\n```\n### Model Calls\nThese methods implement the proper calls to the loaded LLM.\n#### `gemini\\_call(self, content: str)`\nMakes a call to the Gemini model.\n##### Parameters:\n- `content` (`str`): The content to send to the Gemini model.\n##### Returns:\n- `str`: The text response from the Gemini model.\n##### Usage example:\n```python\nresponse = core\\_llm.gemini\\_call(\"Tell me a joke.\")\nprint(response)\n```\n#### `local\\_quantized\\_call(self, content: str)`\nMakes a call to a locally loaded quantized model (e.g., Llama-3.2-3B-Instruct).\n##### Parameters:\n- `content` (`str`): The content to send to the model.\n##### Returns:\n- `str`: The decoded text output from the model.\n##### Functionality:\n1. Tokenizes the input `content` using the loaded tokenizer.\n2. Moves the input tensors to the same device as the model.\n3. Generates output from the model.\n4. Decodes the output using the tokenizer.\n##### Usage example:\n```python\nresponse = core\\_llm.local\\_quantized\\_call(\"Write a short story.\")\nprint(response)\n```\n### Utility Method: `html2markdown(response: str)`\nThis static method converts HTML content to Markdown format.\n#### Parameters:\n- `response` (`str`): The HTML content to convert.\n#### Returns:\n- `str`: The Markdown representation of the input HTML.\n#### Usage example:\n```python\nmarkdown\\_text = CoreLLM.html2markdown(\"\n# Hello World\n\n\")\nprint(markdown\\_text)\n```\n### Callable Interface: `\\_\\_call\\_\\_(self, user\\_content: str)`\nThis method makes the `CoreLLM` object callable like a function. It takes user content, formats it using the prompt template, and calls the appropriate model based on the loaded `model\\_name`.\n#### Parameters:\n- `user\\_content` (`str`): The user-provided content to process.\n#### Returns:\n- `str`: The Markdown-formatted response from the LLM.\n#### Functionality:\n1. Formats the `user\\_content` using the `prompt\\_template`.\n2. Based on the `model\\_name`, calls either `self.gemini\\_call` or `self.local\\_quantized\\_call`.\n3. Converts the response to Markdown using `self.html2markdown`.\n#### Usage example:\n```python\ncore\\_llm.load\\_model(\"Gemini 2.0 Flash\")\nresponse = core\\_llm(\"Tell me about the capital of France.\")\nprint(response)\n```\n## Example Usage\n```python\n# Initialize the CoreLLM with verbose mode\ncore\\_llm = CoreLLM(verbose=False)\n# Load the Gemini 2.0 Flash model\ncore\\_llm.load\\_model(\"Gemini 2.0 Flash\")\n# Define a simple \"hello\" prompt to test the model\nhello\\_prompt = \"Hello, how are you today?\"\n# Call the model with the prompt and get the response\nresponse = core\\_llm(hello\\_prompt)\n# Print the response\nprint(response)\n```\n",
  "utils.py": "```markdown\n# Code Directory Traversal and Structure Preservation\nThis document describes the functionality of the `traverse\\_directory` function, which recursively traverses a directory, reads the content of code files (specified by their extensions), and represents the directory structure and file contents as a JSON string.\n## Function: `traverse\\_directory(root\\_dir)`\n### Description\nThe `traverse\\_directory` function takes a root directory path as input and performs a depth-first traversal to create a structured representation of the directory's contents. This representation includes the content of specified code files, organized according to the original directory structure. Non-code files are ignored.\n### Arguments\n\\* `root\\_dir` (str): The path to the root directory that needs to be traversed.\n### Return Value\n\\* `str`: A JSON string representing the directory structure. Folders are represented as dictionaries where keys are the names of sub-directories or files, and values are nested dictionaries (for sub-directories) or file content (for code files). Empty subdirectories are excluded from the output. Non-code files are excluded too.\n### Functionality Details\nThe `traverse\\_directory` function utilizes two inner functions: `read\\_file` and `traverse`.\n1. \\*\\*`read\\_file(file\\_path)`\\*\\*:\n\\* This inner function takes the path to a file as input.\n\\* It opens the file in read mode ('r') with UTF-8 encoding to handle a wide range of characters.\n\\* It reads the entire content of the file and returns it as a string.\n2. \\*\\*`traverse(current\\_path)`\\*\\*:\n\\* This inner function is the core of the directory traversal logic. It takes the current path (initially the `root\\_dir`) as input and operates recursively.\n\\* \\*\\*If the `current\\_path` is a directory:\\*\\*\n\\* It creates an empty dictionary `folder\\_dict` to store the contents of the directory.\n\\* It iterates through each item (files and subdirectories) within the `current\\_path`.\n\\* For each item, it constructs the full path `item\\_path`.\n\\* It recursively calls `traverse` on `item\\_path` to process the item. The result of this recursive call becomes the value associated with the item's name (the key) in the `folder\\_dict`.\n\\* After processing all items in the directory, it filters `folder\\_dict` to remove any items where the value is `None`. This handles the case of empty directories.\n\\* If the resulting `folder\\_dict` is not empty, it returns the dictionary, representing the directory and its contents. If the directory becomes empty after filtering out `None` values, it returns `None`.\n\\* \\*\\*If the `current\\_path` is a file:\\*\\*\n\\* It extracts the file extension from the `current\\_path`.\n\\* It checks if the file extension is one of the specified code file extensions ('.py', '.js', '.java', '.cpp', '.c').\n\\* If it's a code file, it calls `read\\_file` to read the content of the file and returns the content as a string.\n\\* If it's not a code file, it returns `None`, indicating that the file should be ignored.\nFinally, the `traverse\\_directory` function calls the `traverse` function with the `root\\_dir` to initiate the directory traversal. The result, which is a nested dictionary representing the directory structure and code file contents, is then converted into a JSON string with an indent of 4 spaces for readability using `json.dumps()`, and returned.\n### Error Handling\n\\* The code includes `encoding='utf-8'` when reading files, helping to handle a wider range of characters and avoid encoding-related errors.\n\\* The filtering of empty subdirectories (those resulting in `None`) prevents clutter in the final JSON output.\n### Example Usage\n```python\nimport os\nimport pprint\nimport json\ndef traverse\\_directory(root\\_dir):\ndef read\\_file(file\\_path):\nwith open(file\\_path, 'r', encoding='utf-8') as file:\nreturn file.read()\ndef traverse(current\\_path):\nif os.path.isdir(current\\_path):\nfolder\\_dict = {}\nfor item in os.listdir(current\\_path):\nitem\\_path = os.path.join(current\\_path, item)\nfolder\\_dict[item] = traverse(item\\_path)\n# Remove any empty subfolders (those that lead to None)\nfolder\\_dict = {k: v for k, v in folder\\_dict.items() if v is not None}\nreturn folder\\_dict if folder\\_dict else None\nelse:\n# Assuming code files have extensions like .py, .js, .java, etc.\nfile\\_ext = current\\_path.split('.')[-1]\nif file\\_ext in ['py', 'js', 'java', 'cpp', 'c']:\nreturn read\\_file(current\\_path)\nreturn None # Ignore non-code files\nreturn json.dumps(traverse(root\\_dir), indent=4)\n# Example Usage:\nroot\\_directory = r\"/Users/zhuxiaoai/Desktop/CodeScribe/backend\" # Replace with your desired root directory\ndirectory\\_structure = traverse\\_directory(root\\_directory)\npprint.pprint(directory\\_structure, indent=4)\n```\n\\*\\*Note:\\*\\* Replace `/Users/zhuxiaoai/Desktop/CodeScribe/backend` with the actual path to the directory you want to traverse.\nThe output will be a JSON string, which `pprint.pprint` will display in a human-readable format. For example, it might look like this (the actual content will depend on the structure of the traversed directory):\n```json\n{\n\"module1\": {\n\"file1.py\": \"print('Hello, world!')\\n\",\n\"subdir\": {\n\"file2.py\": \"def my\\_function():\\n return 42\\n\"\n}\n},\n\"file3.js\": \"console.log('Hello from JavaScript!');\\n\"\n}\n```\n"
}